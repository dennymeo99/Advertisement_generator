{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install -U adapter-transformers sentencepiece\n",
    "!pip install datasets\n",
    "%pip install \"accelerate>=0.16.0,<1\" \"transformers[torch]>=4.28.1,<5\" \"torch>=1.13.1,<2\" datasets\n",
    "%pip install diffusers\n",
    "%pip install transformers\n",
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Textual model training\n",
    "#@markdown The first step to perform the model training was, of course, selecting the model itself. <br><br> We wanted to use a model with good performances, but also simple enough to be suitable for our small-sized dataset\n",
    "#@markdown and after several trials, we selected a text-generation model from google, called <i>flan-t5-base.</i><br>This specific model required the training set to be encoded and tokenized in a specific format, with the addition of a padding. This is the operation performed here.\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "base_model = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "prefix = ''\n",
    "\n",
    "# tokenize the dataset\n",
    "def encode_batch(examples):\n",
    "    # the name of the input column\n",
    "    text_column = 'question'\n",
    "    # the name of the target column\n",
    "    summary_column = 'answer'\n",
    "    # used to format the tokens\n",
    "    padding = \"max_length\"\n",
    "\n",
    "    inputs, targets = [], []\n",
    "    for i in range(len(examples[text_column])):\n",
    "        if examples[text_column][i] and examples[summary_column][i]:\n",
    "            inputs.append(examples[text_column][i])\n",
    "            targets.append(examples[summary_column][i])\n",
    "\n",
    "    inputs = [prefix + inp for inp in inputs]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, padding=padding, truncation=True)\n",
    "    labels = tokenizer(targets, max_length=512, padding=padding, truncation=True)\n",
    "\n",
    "    # rename to labels for training\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@markdown The function here defined applies the tokenization step to the training split\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load the dataset\n",
    "def load_split(split_name, max_items):\n",
    "    # load the split\n",
    "    dataset = load_dataset('csv', data_files='/content/preprocessed.csv', delimiter=',')[split_name]\n",
    "    # only use the first max_items items\n",
    "    dataset = dataset.filter(lambda _, idx: idx < max_items, with_indices=True)\n",
    "    # tokenize the dataset\n",
    "    dataset = dataset.map(\n",
    "        encode_batch,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "        desc=\"Running tokenizer on \" + split_name + \" dataset\",\n",
    "    )\n",
    "    # set the format to torch\n",
    "    dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\"])\n",
    "\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@markdown Here, as final step in the training phase, we tune the parameters and load the pretrained model.<br>\n",
    "#@markdown The selected parameters values are the following: <ul><li>learning_rate = 3e-4</li><li>training epochs = 1</li><li>batch size = 1</li></ul><br>\n",
    "#@markdown In addition to this, <i>Low Rank Adaptation (LoRA)</i> configuration was exploited: this is a technique used to simplify the training of this kind of models on specific tasks\n",
    "#@markdown <br> Then, at last, the training phase is started.\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, TrainerCallback\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers.adapters import LoRAConfig\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# start with the pretrained base model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    base_model\n",
    ")\n",
    "\n",
    "# set the parameters for LoRA\n",
    "config = LoRAConfig(\n",
    "    r=8,\n",
    "    alpha=16,\n",
    "    intermediate_lora=True,\n",
    "    output_lora=True\n",
    ")\n",
    "\n",
    "\n",
    "# small batch size to fit in memory\n",
    "batch_size = 1\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=3e-4,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    logging_steps=200,\n",
    "    output_dir=\"./training_output\",\n",
    "    overwrite_output_dir=True,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=load_split(\"train\", 1000),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Inference phase\n",
    "#@markdown After the model has been trained on our dataset, it is time to test its results, asking it to produce an advertisement slogan given the company name and the field where it operates<br><br>\n",
    "#@markdown In addition to this, we wanted to check the quality of the obtained results by comparing our model generated slogans with the ones that can be obtained using the <i>OpenAI GPT-3.5 turbo</i><br><br>\n",
    "#@markdown Use the following boxes to input the company name and the company field\n",
    "\n",
    "company = \"\" #@param {type:\"string\"}\n",
    "field = \"\" #@param {type:\"string\"}\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "import openai\n",
    "\n",
    "input_text = \"What could it be a good advertising slogan for a company called \" + company + \"which operates in the \" + field + \"field?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "input_ids = input_ids.to(device)\n",
    "outputs = model.generate(input_ids)\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": input_text}\n",
    "  ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(input_text)\n",
    "print(\"Custom model output: \" + tokenizer.decode(outputs[0]))\n",
    "print(\"GPT-3.5 turbo output: \" + completion.choices[0].message.content)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
