{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6qYEqrruUySe"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nr-qqzpf6U_a"
   },
   "outputs": [],
   "source": [
    "%pip install -r scraping_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UqEmvlC5Muww",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Scraping module for obtaining the textual dataset\n",
    "#@markdown As the title explains, this module was exploited to scrape from the website \"www.thinkslogans.com\" as many as possible advertisements slogans, which will then be used to train the text-generation model<br>\n",
    "#@markdown This operation was performed using BeatifulSoup to perform the parsing on the requests results. Those requests were directed to several slogan categories available on the website, and along with the slogan, also the brand name and the category were obtained.<br>The last step was to save the result in a .csv file\n",
    "import requests\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "category_list=['airline', 'alcohol', 'bank', 'beverage', 'candy', 'car-brand', 'cereal', 'coffee', 'computers', 'electronic-products', 'fast-food', 'motorcycle', 'perfume', 'toothpaste']\n",
    "max_page_count = 12\n",
    "output_file = 'sloganlist.csv'\n",
    "\n",
    "def remove_html_tags(text):\n",
    "  clean = re.compile(r\"<.*?>| \\n |\\[|\\]|â€“|, | \\'\")\n",
    "  return re.sub(clean, '', text)\n",
    "\n",
    "\n",
    "\n",
    "def get_data(url):\n",
    "  # In case of Status 403 (Forbidden), wait for some time (maybe hours) before retrying\n",
    "  headers = {\n",
    "  \"Connection\": \"keep-alive\",\n",
    "  \"Upgrade-Insecure-Requests\": \"1\",\n",
    "  \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.106 Safari/537.36\",\n",
    "  \"Sec-Fetch-Dest\": \"document\",\n",
    "  \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "  \"Sec-Fetch-Site\": \"none\",\n",
    "  \"Sec-Fetch-Mode\": \"navigate\",\n",
    "  \"Sec-Fetch-User\": \"?1\",\n",
    "  \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "  \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "  }\n",
    "\n",
    "  try:\n",
    "    return requests.get(url, headers=headers).text\n",
    "  except Exception:\n",
    "    time.sleep(1)\n",
    "    return requests.get(url, headers=headers).text\n",
    "  print(requests.get(url).status_code)\n",
    "\n",
    "def collect_data(category_list=None, max_page_count=0):\n",
    "  # a. scrape data from the link\n",
    "  # b. parse it\n",
    "  # c. final result stored as a list of rows\n",
    "  rows=[]\n",
    "  print(max_page_count)\n",
    "  print(category_list)\n",
    "  for category in category_list:\n",
    "    print('Category:'+category)\n",
    "    base_url = \"https://www.thinkslogans.com/slogans/advertising-slogans/\"+str(category)+\"-slogans/\"\n",
    "    for page in range(max_page_count):\n",
    "      page = page + 1\n",
    "      url = base_url\n",
    "      if(page != 1):\n",
    "        url = base_url + \"page/\"+str(page)+\"/\"\n",
    "      print(\"Page Number:\"+str(page))\n",
    "      \n",
    "      response = requests.head(url)\n",
    "      \n",
    "      #if response.status_code != 200:\n",
    "        #break\n",
    "      data = get_data(url)\t\t\t\n",
    "      soup = BeautifulSoup(data,'html.parser')\n",
    "      #org_names = soup.findAll('h5',{'class':'list-group-item-heading'})\n",
    "      org_slogans = soup.findAll('div',{'class':'entry'})\n",
    "      org_slogans = list(str(org_slogans).split(\"<\\div>\"))\n",
    "      for i in range(0,len(org_slogans)):\n",
    "        try:\n",
    "            raw_line = remove_html_tags(org_slogans[i])\n",
    "            print(raw_line)\n",
    "            raw_line = (raw_line.strip()).split(\"\\n\\n\")\n",
    "            raw_line = [x for x in raw_line if x != \" \" and x != \"\"]\n",
    "            for i in range(0,len(raw_line)):\n",
    "                if(\"\\n\" in raw_line[i]):\n",
    "                  row = [raw_line[i].split(\"\\n\")[0], category, raw_line[i].split(\"\\n\")[1]]\n",
    "                  rows.append(row)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "  return rows\n",
    "\n",
    "def write_data(data, output_file):\n",
    "  \n",
    "  # write data to the output file\n",
    "  with open(output_file, 'w', newline='') as file:\n",
    "    print(data)\n",
    "    writer = csv.writer(file)\n",
    "    # header of the csv\n",
    "    writer.writerow(['Slogan', 'Category', 'Company'])\n",
    "    # contents\n",
    "    writer.writerows(data)\n",
    "  pass\n",
    "\n",
    "def main():\n",
    "  data = collect_data(category_list, max_page_count)\n",
    "  write_data(data, output_file)\n",
    "  return 0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  sys.exit(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NpikgLDW3u_m",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#@markdown The \"slogans.csv\" file still had to be preprocessed before its usage: for this reason, another step had to be carried out. <br> Since we want our model to be trained on a question-response type of task, \n",
    "#@markdown the dataset had to be composed by questions, which are of the type <i>\"What could it be a good advertising slogan for a company called \" + <b>company name</b> + \" which operates in the \" + <b>company category</b> + \" field?\"</i><br> The response will be the slogan itself\n",
    "\n",
    "dataset = pd.read_csv(\"/content/drive/MyDrive/sloganlist.csv\")\n",
    "model_friendly_dataset = pd.DataFrame(columns = [\"question\", \"answer\"])\n",
    "\n",
    "for i in range (0, len(dataset)):\n",
    "  row = dataset.iloc[i]\n",
    "  question_string = \"What could it be a good advertising slogan for a company called \" + row['Company'] + \" which operates in the \" + row['Category'] + \" field?\"\n",
    "  model_friendly_dataset.loc[i] = [question_string, row['Slogan']]\n",
    "\n",
    "model_friendly_dataset.to_csv(\"preprocessed.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "QdV52erd2-fB"
   },
   "outputs": [],
   "source": [
    "#@title Scraping for the image dataset\n",
    "#@markdown Moving on the second part, we wanted to see if an image generation model could be able to produce an advertisement banner\n",
    "#@markdown starting from the same parameters for the textual part, adding the slogan produced by the previous model to the input prompt. In order to do this, we also decided to use the following code to scrape around 5000 advertisement images\n",
    "\n",
    "import requests\n",
    "\n",
    "for i in range(0, 5):\n",
    "  j = 0\n",
    "  for j in range (0, 100000):\n",
    "    base_url = f'https://people.cs.pitt.edu/~mzhang/image_ads/{i}/{j}.jpg'\n",
    "    if(requests.get(base_url).status_code != 404):\n",
    "      print(\"Immagine ottenuta: \" + base_url)\n",
    "      img_data = requests.get(base_url).content\n",
    "      filepath = \"/content/advertisements/ad_\" + str(i) + \"_\" + str(j) + \".jpg\" \n",
    "      with open(filepath, 'wb') as handler:\n",
    "          handler.write(img_data)\n",
    "    else:\n",
    "      print(\"immagine non ritrovata: \" + base_url)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
